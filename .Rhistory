## Social Issues ##
Child bullying â€“ when will it end?
git status
# Replace 'file_path' with the path to your CSV file
file_path <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2019_part_1.csv"
# Read the first 1000 rows of the CSV file
data <- read.csv(file_path)
# View the first few rows of the data
head(data)
View(data)
gc()
gc()
# Replace 'file_path' with the path to your CSV file
file_path <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/test_2019_part_1.csv"
# Read the first 1000 rows of the CSV file
data2 <- read.csv(file_path)
View(data2)
# Print 'actor_id' if it is 1
actor_ids <- data2$actor_id[data2$actor_id == 1]
print(actor_ids)
# Print 'actor_id' if it is 1
actor_ids <- data2$actor_id[data2$actor_id == 4]
print(actor_ids)
# Get unique actor_ids from data
actor_ids_data <- unique(data$actor_id)
# Get unique actor_ids from data2
actor_ids_data2 <- unique(data2$actor_id)
# Find actor_ids present in data but not in data2
missing_ids <- setdiff(actor_ids_data, actor_ids_data2)
# Print the missing actor_ids
if (length(missing_ids) > 0) {
print("Actor IDs present in data but not in data2:")
print(missing_ids)
} else {
print("All actor IDs in data are also present in data2.")
}
# Count the number of rows in data with missing IDs
num_rows_with_missing_ids <- sum(data$actor_id %in% missing_ids)
# Print the count
print(paste("Number of rows with missing IDs in data:", num_rows_with_missing_ids))
# Get unique actor_ids from data
actor_ids_data <- unique(data$actor_id)
# Get unique actor_ids from data2
actor_ids_data2 <- unique(data2$actor_id)
# Find actor_ids present in data but not in data2
missing_ids <- setdiff(actor_ids_data, actor_ids_data2)
# Print the missing actor_ids
if (length(missing_ids) > 0) {
print("Actor IDs present in data but not in data2:")
print(missing_ids)
} else {
print("All actor IDs in data are also present in data2.")
}
# Count the number of rows in data with missing IDs
num_rows_with_missing_ids <- sum(data$actor_id %in% missing_ids)
# Print the count
print(paste("Number of rows with missing IDs in data:", num_rows_with_missing_ids))
# Get unique actor_ids from data2
actor_ids_data2 <- unique(data2$actor_id)
# Find actor_ids present in both data and data2
matching_ids <- intersect(actor_ids_data, actor_ids_data2)
# Count the number of rows in data with IDs present in data2
num_rows_with_matching_ids <- sum(data$actor_id %in% matching_ids)
# Print the count
print(paste("Number of rows with IDs in data2 in data:", num_rows_with_matching_ids))
gc()
gc()
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path1 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_2019_part_1.csv"
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2019_part_1.csv"
# Read the CSV files
data <- read.csv(file_path1)
data2 <- read.csv(file_path2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path1 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_2019_part_1.csv"
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2019_part_1.csv"
# Read the CSV files
data <- read.csv(file_path1)
data2 <- read.csv(file_path2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2023_part_1.csv"
# Read the CSV files
data2 <- read.csv(file_path2)
View(data2)
colnames(df)
colnames(data2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2023_part_1.csv"
# Read the CSV files
data2 <- read.csv(file_path2)
View(data2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2023_part_1.csv"
# Read the CSV files
data2 <- read.csv(file_path2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path2 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/github_day_location_2023_part_1.csv"
# Read the CSV files
data2 <- read.csv(file_path2, nrows=100)
View(data2)
# Load dplyr package
library(dplyr)
# Replace 'file_path' with the path to your CSV files
file_path1 <- "C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/test_github_day_repo_location_2019_part_1.csv"
data <- read.csv(file_path1)
View(data)
gc()
# Read the CSV files
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2019.csv")
adm2_action_year_2019 <- read.csv("C:/Users/Tze Yong Tan/Downloads/github_activities_2019_2023 - Copy/adm2_action_year_2019.csv")
View(github_location_day_2019)
View(adm2_action_year_2019)
# Load necessary library
library(dplyr)
# Read the CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2019.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_user_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
View(github_user_day_2019)
# Load necessary library
library(dplyr)
# Read the CSV file
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2019.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_location_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the CSV file
adm2_action_year_2019 <- read.csv("C:/Users/Tze Yong Tan/Downloads/github_activities_2019_2023 - Copy/adm2_action_year_2019.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- adm2_action_year_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the CSV file
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2020.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_location_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the CSV file
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2021.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_location_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the CSV file
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2022.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_location_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the CSV file
github_location_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/location_day_analysis/github_location_day_2023.csv")
# Check if adm2_geonamesId appears only once for each date
duplicate_check <- github_location_day_2019 %>%
group_by(date, adm2_geonamesId) %>%
summarise(count = n()) %>%
filter(count > 1)
# Print the results
if(nrow(duplicate_check) == 0) {
print("Each adm2_geonamesId appears only once for each date.")
} else {
print("There are adm2_geonamesId that appear more than once for some dates:")
print(duplicate_check)
}
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2019.csv")
# Initialize an empty data frame to hold the combined data
combined_parts <- data.frame()
# Loop through part numbers 1 to 5 and read each file
for (partnumber in 1:5) {
file_path <- paste0("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_location_day_2019_part", partnumber, ".csv")
part_data <- read.csv(file_path)
# Combine the part data frames
combined_parts <- bind_rows(combined_parts, part_data)
}
# Compare rows between the main file and the combined parts
# Check for rows in main file not in combined parts
not_in_combined <- anti_join(github_user_day_2019, combined_parts, by = colnames(github_user_day_2019))
# Check for rows in combined parts not in main file
not_in_main <- anti_join(combined_parts, github_user_day_2019, by = colnames(github_user_day_2019))
# Print the results
if(nrow(not_in_combined) == 0) {
print("All rows in the main file are present in the combined parts.")
} else {
print("Rows in the main file that are not in the combined parts:")
print(not_in_combined)
}
if(nrow(not_in_main) == 0) {
print("All rows in the combined parts are present in the main file.")
} else {
print("Rows in the combined parts that are not in the main file:")
print(not_in_main)
}
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2020.csv")
# Initialize an empty data frame to hold the combined data
combined_parts <- data.frame()
# Loop through part numbers 1 to 5 and read each file
for (partnumber in 1:5) {
file_path <- paste0("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_location_day_2020_part", partnumber, ".csv")
part_data <- read.csv(file_path)
# Combine the part data frames
combined_parts <- bind_rows(combined_parts, part_data)
}
# Compare rows between the main file and the combined parts
# Check for rows in main file not in combined parts
not_in_combined <- anti_join(github_user_day_2019, combined_parts, by = colnames(github_user_day_2019))
# Check for rows in combined parts not in main file
not_in_main <- anti_join(combined_parts, github_user_day_2019, by = colnames(github_user_day_2019))
# Print the results
if(nrow(not_in_combined) == 0) {
print("All rows in the main file are present in the combined parts.")
} else {
print("Rows in the main file that are not in the combined parts:")
print(not_in_combined)
}
if(nrow(not_in_main) == 0) {
print("All rows in the combined parts are present in the main file.")
} else {
print("Rows in the combined parts that are not in the main file:")
print(not_in_main)
}
gc()
gc()
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2021.csv")
# Initialize an empty data frame to hold the combined data
combined_parts <- data.frame()
# Loop through part numbers 1 to 5 and read each file
for (partnumber in 1:5) {
file_path <- paste0("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_location_day_2021_part", partnumber, ".csv")
part_data <- read.csv(file_path)
# Combine the part data frames
combined_parts <- bind_rows(combined_parts, part_data)
}
# Compare rows between the main file and the combined parts
# Check for rows in main file not in combined parts
not_in_combined <- anti_join(github_user_day_2019, combined_parts, by = colnames(github_user_day_2019))
# Check for rows in combined parts not in main file
not_in_main <- anti_join(combined_parts, github_user_day_2019, by = colnames(github_user_day_2019))
# Print the results
if(nrow(not_in_combined) == 0) {
print("All rows in the main file are present in the combined parts.")
} else {
print("Rows in the main file that are not in the combined parts:")
print(not_in_combined)
}
if(nrow(not_in_main) == 0) {
print("All rows in the combined parts are present in the main file.")
} else {
print("Rows in the combined parts that are not in the main file:")
print(not_in_main)
}
gc()
gc()
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2022.csv")
# Initialize an empty data frame to hold the combined data
combined_parts <- data.frame()
# Loop through part numbers 1 to 5 and read each file
for (partnumber in 1:5) {
file_path <- paste0("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_location_day_2022_part", partnumber, ".csv")
part_data <- read.csv(file_path)
# Combine the part data frames
combined_parts <- bind_rows(combined_parts, part_data)
}
gc()
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_day_2023.csv")
# Initialize an empty data frame to hold the combined data
combined_parts <- data.frame()
# Loop through part numbers 1 to 5 and read each file
for (partnumber in 1:5) {
file_path <- paste0("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_location_day_analysis/github_user_location_day_2023_part", partnumber, ".csv")
part_data <- read.csv(file_path)
# Combine the part data frames
combined_parts <- bind_rows(combined_parts, part_data)
}
gc()
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_repo_location_day_analysis/github_user_repo_location_day_2019_part1.csv")
# Load necessary library
library(dplyr)
# Read the main CSV file
github_user_day_2019 <- read.csv("C:/Users/Tze Yong Tan/Desktop/github_activities_2019_2023/Data/user_repo_location_day_analysis/github_user_repo_location_day_2019_part1.csv", nrows=10)
#
View(github_user_day_2019)
